# Project 1 â€“ Evaluation Harness for Agents  
_Measuring Agent Performance with Explicit Rubrics_

This project implements a **systematic evaluation framework** for AI agents, enabling repeatable, rubric-driven assessment of agent outputs across tasks, runs, and configurations.

It establishes a reliable baseline for comparing agents before introducing introspection or behavior updates.

---

## Why This Project Exists

Most agent systems are evaluated using:
- informal prompts
- subjective judgments
- single-run examples

This makes it impossible to answer basic questions like:
- Is the agent actually improving?
- Which version is better?
- What trade-offs are we making?

This project treats **evaluation as infrastructure**, not an afterthought.

---
